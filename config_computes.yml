---

#This playbook is temporary until the testing for building a base image
# and then switching to create/destroy is complete!
#  Primarily for testing the public-ip NFS mounts, etc.
#
#
- hosts: meta-compute_*
  tasks:

  - name: Get the slurmctld uid
    local_action:
      module: shell getent passwd slurm | awk -F':' '{print $3}'
    register: headnode_slurm_uid
    become: False # for running as slurm, since no sudo on localhost

  - name: Add OpenHPC 1.3.5 repo
    yum_repository:
      name: OpenHPC
      description: OpenHPC 1.3.5 repo
      file: OpenHPC
      baseurl: http://build.openhpc.community/OpenHPC:/1.3:/Update5/CentOS_7/
      gpgkey: http://build.openhpc.community/OpenHPC:/1.3:/Update5/CentOS_7/repodata/repomd.xml.key
      gpgcheck: yes

  - name: install basic packages
    yum: 
      name: "{{ compute-base-package-list }}" 
      state: present
    with_vars:
      compute-base-package-list:
        - "openmpi"       
        - "libselinux-python"
        - "telnet"
        - "bind-utils"
        - "vim"
        - "ohpc-slurm-client"
 #    - "quantum-espresso-openmpi"
 #    - "quantum-espresso"
 #     - "rsync"
 #     - "epel-release"
 #     - "openmpi-devel"       #torque
 #     - "gcc"           
 #     - "gcc-c++"       
 #     - "gcc-gfortran"  
 #     - "openssl-devel" 
 #     - "libxml2-devel" 
 #     - "boost-devel"   
 #     - "net-tools"
 #     - "strace"
 #     - "wget"  # needed for building QE
 #     - "readline-devel"  #req for slurm
 #     - "pam-devel"       # req for slurm
 #     - "perl-ExtUtils-MakeMaker" # req for slurm
 #     - "fftw" # req for QE... need a better way to specify these!!!
 #
# - name: set up /etc/hosts
#   template: src=hosts.j2 dest=/etc/hosts

  - name: fix slurm user uid
    user:
      name: slurm
      uid: "{{ headnode_slurm_uid.stdout}}"
      shell: "/sbin/nologin"
      home: "/etc/slurm"

  - name: change ownership of slurm files
    file:
      path: "{{ item }}"
      owner: slurm
      group: slurm
    with_items:
      - "/var/log/slurm_jobacct.log"
      - "/var/spool/slurm"
      - "/var/spool/slurm/ctld"

 # - name: export /home to compute nodes
 #   lineinfile:
 #     dest: /etc/exports
 #     line:  "/home 10.0.0.0/24(rw,no_root_squash)"
 #     state: present
 #   tags:
 #     - export_home
 #   when: submit_host == "yes"
 
 # - name: allow all to write /export
 #   file: 
 #     path: /export/
 #     state: directory
 #     mode: 0777
 #
 # - name: export /export to compute nodes
 #   lineinfile:
 #     dest: /etc/exports
 #     line:  "/export 10.0.0.0/24(rw,no_root_squash)"
 #     state: present
 #   tags:
 #     - export_N
 #   when: submit_host == "yes"
 
 # - name: allow use_nfs_home_dirs
 #   seboolean: name=use_nfs_home_dirs state=yes persistent=yes
 #
 # - name: restart nfs
 #   service: name=nfs state=restarted
 #   when: (submit_host == "yes" and cleanup == "no")
  
  
 # - name: import /home on compute nodes
 #   lineinfile:
 #     dest: /etc/fstab
 #     line:  "{{ headnode_private_ip.stdout }}:/home  /home  nfs  defaults,nfsvers=4.0 0 0"
 #     state: present
 #   tags:
 #     - export_home

  - name: ensure /export exists
    file: path=/export state=directory mode=777
 
  - name: import /export on compute nodes
    lineinfile:
      dest: /etc/fstab
      line:  "{{ headnode_public_ip }}:/export  /export  nfs  defaults,nfsvers=4.0 0 0"
      state: present
 
  - name: add local users to compute node
    script: /tmp/add_users.sh
    ignore_errors: True

  - name: copy munge key from headnode
    synchronize:
      mode: push
      src: /etc/slurm/.munge.key
      dest: /etc/munge/munge.key
      set_remote_user: no
      use_ssh_args: yes

  - name: fix perms on munge key
    file: 
      path: /etc/munge/munge.key
      owner: munge
      group: munge
      mode: 0600
 
  - name: copy slurm.conf from headnode
    synchronize:
      mode: push
      src: /etc/slurm/slurm.conf
      dest: /etc/slurm/slurm.conf
      set_remote_user: no
      use_ssh_args: yes
 
  - name: copy slurm_prolog.sh from headnode
    synchronize:
      mode: push
      src: /usr/local/sbin/slurm_prolog.sh
      dest: /usr/local/sbin/slurm_prolog.sh
      set_remote_user: no
      use_ssh_args: yes
 
  - name: enable and start munge
    service: name=munge.service enabled=yes state=started
 
  - name: start slurmd
    service: name=slurmd state=restarted enabled=yes

  - name: mount -a on compute nodes
    command: "mount -a"
